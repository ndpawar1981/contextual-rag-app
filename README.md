<h1 align="center">ğŸ§  Contextual RAG Assistant</h1>

<p align="center">
  <b>A Modular, Context-Aware Retrieval Augmented Generation System Built with LangChain, ChromaDB, OpenAI & Streamlit</b>
  <br/>
  <sub>Featuring contextual chunking, citations, sources, and a beautiful streaming Chat UI.</sub>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10+-blue" />
  <img src="https://img.shields.io/badge/Framework-LangChain-green" />
  <img src="https://img.shields.io/badge/Frontend-Streamlit-red" />
  <img src="https://img.shields.io/badge/VectorDB-ChromaDB-purple" />
  <img src="https://img.shields.io/badge/License-MIT-yellow" />
</p>

---

## ğŸš€ Overview

**Contextual RAG Assistant** is a full-stack, production-ready RAG (Retrieval Augmented Generation) application that:

âœ” Loads and processes PDFs  
âœ” Generates **context-enriched chunks using an LLM**  
âœ” Stores embeddings in **ChromaDB**  
âœ” Retrieves top-K relevant chunks  
âœ” Generates answers with **citations**, **sources**, and **justifications**  
âœ” Provides a **modern Streamlit UI** with a **typing / character stream effect**  
âœ” Fully modular â€” loader, retriever, generator, and UI separated into clean Python modules  

This project converts a notebook into a scalable codebase suitable for real production RAG systems.

---

## ğŸ“ Project Structure

ğŸ“¦ rag_app/
â”‚
â”œâ”€â”€ app.py # Streamlit UI with streaming chat output
â”œâ”€â”€ config.py # Centralized configuration (models, paths, chunk size, etc.)
â”œâ”€â”€ loader.py # Document loader + contextual chunk generator + vector store builder
â”œâ”€â”€ retriever.py # ChromaDB retriever wrapper
â”œâ”€â”€ generator.py # RAG pipelines: basic, with sources, with citations
â”‚
â”œâ”€â”€ requirements.txt # All dependencies
â””â”€â”€ README.md # This file

